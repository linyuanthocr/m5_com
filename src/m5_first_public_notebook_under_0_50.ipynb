{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "m5-first-public-notebook-under-0.50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "drGRQyCt3ON7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  datetime import datetime, timedelta\n",
        "import gc\n",
        "import numpy as np, pandas as pd\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvVMviE83OOA",
        "colab_type": "text"
      },
      "source": [
        "> This notebook aims to push the public LB under 0.50. Certainly, the competition is not yet at its peak and there clearly remains room for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzVyAhow3OOA",
        "colab_type": "text"
      },
      "source": [
        "# Credits\n",
        "\n",
        "* [First R notebook](https://www.kaggle.com/kailex/m5-forecaster-v2)\n",
        "* [Python translation](https://www.kaggle.com/kneroma/m5-forecast-v2-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W5_2ViF3OOB",
        "colab_type": "text"
      },
      "source": [
        "# Changes\n",
        "* v5 : try to optimise the LGBM params (go below in lgbm params section to see changes)\n",
        "* v4 : add df, X_train deletion before training step --> increasing train sample without memeroy issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-E2JR_k3OOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Kaggle = False\n",
        "Colab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-lb5OVI3kq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8Zy0eO3nTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    path = \"/content/drive/My Drive\"\n",
        "\n",
        "    os.chdir(path)\n",
        "    os.listdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma_eB7Ec3qPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if Kaggle:\n",
        "    PATH = '/kaggle/input/'\n",
        "    outdir = '.'\n",
        "# PATH = '/Users/helen/Desktop/Data/'\n",
        "else:\n",
        "    PATH = 'm5_competition/'\n",
        "    outdir = Path(PATH+'res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    # outdir = Path(PATH+'res/wavenet-dlr-res')\n",
        "    # if not os.path.exists(outdir):\n",
        "    #     os.mkdir(outdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6yvqSaW3OOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
        "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
        "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"int16\", 'snap_TX': 'int16', 'snap_WI': 'int16' }\n",
        "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBhWX_v23OOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_columns = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "xxqFhyQ43OOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = 28 \n",
        "max_lags = 57\n",
        "tr_last = 1913\n",
        "fday = datetime(2016,4, 25) \n",
        "fday"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcxJ9zq4706k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame,\n",
        "                     verbose: bool = True) -> pd.DataFrame:\n",
        "    \n",
        "    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if (c_min > np.iinfo(np.int8).min\n",
        "                      and c_max < np.iinfo(np.int8).max):\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif (c_min > np.iinfo(np.int16).min\n",
        "                      and c_max < np.iinfo(np.int16).max):\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif (c_min > np.iinfo(np.int32).min\n",
        "                      and c_max < np.iinfo(np.int32).max):\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif (c_min > np.iinfo(np.int64).min\n",
        "                      and c_max < np.iinfo(np.int64).max):\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float16).min\n",
        "                        and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif (c_min > np.finfo(np.float32).min\n",
        "                      and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    reduction = (start_mem - end_mem) / start_mem\n",
        "\n",
        "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "kUx2oh2H3OOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjVfgH-R3OOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dt(is_train = True, nrows = None, first_day = 1200):\n",
        "    prices = pd.read_csv(PATH+ \"m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n",
        "    for col, col_dtype in PRICE_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
        "            prices[col] -= prices[col].min()\n",
        "            \n",
        "    cal = pd.read_csv(PATH+\"m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n",
        "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
        "    for col, col_dtype in CAL_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
        "            cal[col] -= cal[col].min()\n",
        "    \n",
        "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
        "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
        "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
        "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
        "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
        "    dt = pd.read_csv(PATH+\"m5-forecasting-accuracy/sales_train_validation.csv\", \n",
        "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n",
        "    \n",
        "    for col in catcols:\n",
        "        if col != \"id\":\n",
        "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
        "            dt[col] -= dt[col].min()\n",
        "    \n",
        "    if not is_train:\n",
        "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
        "            dt[f\"d_{day}\"] = np.nan\n",
        "    \n",
        "    dt = pd.melt(dt,\n",
        "                  id_vars = catcols,\n",
        "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
        "                  var_name = \"d\",\n",
        "                  value_name = \"sales\")\n",
        "    \n",
        "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
        "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
        "    \n",
        "    return dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtJZTQOR3OOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fea(dt):\n",
        "    lags = [7, 28, 91, 182]\n",
        "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
        "    for lag, lag_col in zip(lags, lag_cols):\n",
        "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
        "\n",
        "    wins = [7, 28, 91, 182]\n",
        "    for win in wins :\n",
        "        for lag,lag_col in zip(lags, lag_cols):\n",
        "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
        "\n",
        "    \n",
        "    \n",
        "    date_features = {\n",
        "        \n",
        "        \"wday\": \"weekday\",\n",
        "        \"week\": \"weekofyear\",\n",
        "        \"month\": \"month\",\n",
        "        \"quarter\": \"quarter\",\n",
        "        \"year\": \"year\",\n",
        "        \"mday\": \"day\",\n",
        "#         \"ime\": \"is_month_end\",\n",
        "#         \"ims\": \"is_month_start\",\n",
        "    }\n",
        "    \n",
        "#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
        "    \n",
        "    for date_feat_name, date_feat_func in date_features.items():\n",
        "        if date_feat_name in dt.columns:\n",
        "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
        "        else:\n",
        "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ERS5CKD3OOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIRST_DAY = 350 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmC9VHwP3OOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
        "df = reduce_mem_usage(df)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbQzufa33OOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLgaLG1k3OOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoKmE-WU3OOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYdl_CW73OOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "create_fea(df)\n",
        "df.shape\n",
        "df = reduce_mem_usage(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0hHtCbV3OOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h71C5fJ73OOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Y-Dk5z3OOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace = True)\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5NtC_63OOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
        "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n",
        "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
        "X_train = df[train_cols]\n",
        "y_train = df[\"sales\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGvf1InJ3OOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
        "# fake_valid_inds = np.random.choice(len(X_train), 1000000, replace = False)\n",
        "# fake_valid_data = lgb.Dataset(X_train.iloc[fake_valid_inds], label = y_train.iloc[fake_valid_inds],categorical_feature=cat_feats,\n",
        "#                              free_raw_data=False)   # This is just a subsample of the training set, not a real validation set !"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68r3aEe3OOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "np.random.seed(777)\n",
        "\n",
        "fake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
        "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
        "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
        "                         categorical_feature=cat_feats, free_raw_data=False)\n",
        "fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
        "                              categorical_feature=cat_feats,\n",
        "                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y14x2Rdp3OOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvyadn_o3OOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "        \"objective\" : \"poisson\",\n",
        "        \"metric\" :\"rmse\",\n",
        "        \"force_row_wise\" : True,\n",
        "        \"learning_rate\" : 0.075,\n",
        "#         \"sub_feature\" : 0.8,\n",
        "        \"sub_row\" : 0.75,\n",
        "        \"bagging_freq\" : 1,\n",
        "        \"lambda_l2\" : 0.1,\n",
        "#         \"nthread\" : 4\n",
        "        \"metric\": [\"rmse\"],\n",
        "    'verbosity': 1,\n",
        "    'num_iterations' : 1200,\n",
        "    'num_leaves': 128,\n",
        "    \"min_data_in_leaf\": 100,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHD5_hYe3OO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f__Ia6z3OO2",
        "colab_type": "code",
        "outputId": "a4bcdd12-fa8a-49e7-a33f-6f990ef9e855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=20) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[20]\tvalid_0's rmse: 2.86049\n",
            "[40]\tvalid_0's rmse: 2.54933\n",
            "[60]\tvalid_0's rmse: 2.46711\n",
            "[80]\tvalid_0's rmse: 2.44433\n",
            "[100]\tvalid_0's rmse: 2.43292\n",
            "[120]\tvalid_0's rmse: 2.42233\n",
            "[140]\tvalid_0's rmse: 2.41541\n",
            "[160]\tvalid_0's rmse: 2.4071\n",
            "[180]\tvalid_0's rmse: 2.39727\n",
            "[200]\tvalid_0's rmse: 2.39029\n",
            "[220]\tvalid_0's rmse: 2.38436\n",
            "[240]\tvalid_0's rmse: 2.37583\n",
            "[260]\tvalid_0's rmse: 2.37075\n",
            "[280]\tvalid_0's rmse: 2.36538\n",
            "[300]\tvalid_0's rmse: 2.36187\n",
            "[320]\tvalid_0's rmse: 2.3593\n",
            "[340]\tvalid_0's rmse: 2.35543\n",
            "[360]\tvalid_0's rmse: 2.35206\n",
            "[380]\tvalid_0's rmse: 2.34973\n",
            "[400]\tvalid_0's rmse: 2.34645\n",
            "[420]\tvalid_0's rmse: 2.34387\n",
            "[440]\tvalid_0's rmse: 2.34201\n",
            "[460]\tvalid_0's rmse: 2.33843\n",
            "[480]\tvalid_0's rmse: 2.3367\n",
            "[500]\tvalid_0's rmse: 2.33359\n",
            "[520]\tvalid_0's rmse: 2.33105\n",
            "[540]\tvalid_0's rmse: 2.32827\n",
            "[560]\tvalid_0's rmse: 2.32621\n",
            "[580]\tvalid_0's rmse: 2.32409\n",
            "[600]\tvalid_0's rmse: 2.32119\n",
            "[620]\tvalid_0's rmse: 2.31897\n",
            "[640]\tvalid_0's rmse: 2.3165\n",
            "[660]\tvalid_0's rmse: 2.31495\n",
            "[680]\tvalid_0's rmse: 2.31227\n",
            "[700]\tvalid_0's rmse: 2.31028\n",
            "[720]\tvalid_0's rmse: 2.30797\n",
            "[740]\tvalid_0's rmse: 2.30719\n",
            "[760]\tvalid_0's rmse: 2.30616\n",
            "[780]\tvalid_0's rmse: 2.30504\n",
            "[800]\tvalid_0's rmse: 2.30373\n",
            "[820]\tvalid_0's rmse: 2.30165\n",
            "[840]\tvalid_0's rmse: 2.30061\n",
            "[860]\tvalid_0's rmse: 2.29745\n",
            "[880]\tvalid_0's rmse: 2.29518\n",
            "[900]\tvalid_0's rmse: 2.29344\n",
            "[920]\tvalid_0's rmse: 2.29306\n",
            "[940]\tvalid_0's rmse: 2.29252\n",
            "[960]\tvalid_0's rmse: 2.29099\n",
            "[980]\tvalid_0's rmse: 2.28837\n",
            "[1000]\tvalid_0's rmse: 2.28744\n",
            "[1020]\tvalid_0's rmse: 2.28598\n",
            "[1040]\tvalid_0's rmse: 2.28475\n",
            "[1060]\tvalid_0's rmse: 2.28274\n",
            "[1080]\tvalid_0's rmse: 2.28143\n",
            "[1100]\tvalid_0's rmse: 2.2801\n",
            "[1120]\tvalid_0's rmse: 2.27893\n",
            "[1140]\tvalid_0's rmse: 2.27796\n",
            "[1160]\tvalid_0's rmse: 2.27708\n",
            "[1180]\tvalid_0's rmse: 2.27643\n",
            "[1200]\tvalid_0's rmse: 2.27517\n",
            "CPU times: user 4h 51min 58s, sys: 22.7 s, total: 4h 52min 21s\n",
            "Wall time: 1h 18min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfSqC2lI3OO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXUKBMZW3OO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "785ef56c-1bb3-4724-dea3-4bb2746011dc"
      },
      "source": [
        "m_lgb.save_model(os.path.join(outdir,\"model-fd{}.lgb\".format(FIRST_DAY)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7f7d541ae080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLjRm0tG3OO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74c9e8f2-cb6c-46a7-ef6f-3f60b254d841"
      },
      "source": [
        "%%time\n",
        "\n",
        "alphas = [1.028, 1.023, 1.018]\n",
        "weights = [1/len(alphas)]*len(alphas)\n",
        "sub = 0.\n",
        "\n",
        "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
        "\n",
        "    te = create_dt(False)\n",
        "    cols = [f\"F{i}\" for i in range(1,29)]\n",
        "\n",
        "    for tdelta in range(0, 28):\n",
        "        day = fday + timedelta(days=tdelta)\n",
        "        print(tdelta, day)\n",
        "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
        "        create_fea(tst)\n",
        "        tst = tst.loc[tst.date == day , train_cols]\n",
        "        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
        "\n",
        "\n",
        "\n",
        "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
        "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
        "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
        "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
        "    te_sub.fillna(0., inplace = True)\n",
        "    te_sub.sort_values(\"id\", inplace = True)\n",
        "    te_sub.reset_index(drop=True, inplace = True)\n",
        "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
        "    if icount == 0 :\n",
        "        sub = te_sub\n",
        "        sub[cols] *= weight\n",
        "    else:\n",
        "        sub[cols] += te_sub[cols]*weight\n",
        "    print(icount, alpha, weight)\n",
        "\n",
        "\n",
        "sub2 = sub.copy()\n",
        "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
        "sub.to_csv(os.path.join(outdir,\"submission.csv\"),index=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2016-04-25 00:00:00\n",
            "1 2016-04-26 00:00:00\n",
            "2 2016-04-27 00:00:00\n",
            "3 2016-04-28 00:00:00\n",
            "4 2016-04-29 00:00:00\n",
            "5 2016-04-30 00:00:00\n",
            "6 2016-05-01 00:00:00\n",
            "7 2016-05-02 00:00:00\n",
            "8 2016-05-03 00:00:00\n",
            "9 2016-05-04 00:00:00\n",
            "10 2016-05-05 00:00:00\n",
            "11 2016-05-06 00:00:00\n",
            "12 2016-05-07 00:00:00\n",
            "13 2016-05-08 00:00:00\n",
            "14 2016-05-09 00:00:00\n",
            "15 2016-05-10 00:00:00\n",
            "16 2016-05-11 00:00:00\n",
            "17 2016-05-12 00:00:00\n",
            "18 2016-05-13 00:00:00\n",
            "19 2016-05-14 00:00:00\n",
            "20 2016-05-15 00:00:00\n",
            "21 2016-05-16 00:00:00\n",
            "22 2016-05-17 00:00:00\n",
            "23 2016-05-18 00:00:00\n",
            "24 2016-05-19 00:00:00\n",
            "25 2016-05-20 00:00:00\n",
            "26 2016-05-21 00:00:00\n",
            "27 2016-05-22 00:00:00\n",
            "0 1.028 0.3333333333333333\n",
            "0 2016-04-25 00:00:00\n",
            "1 2016-04-26 00:00:00\n",
            "2 2016-04-27 00:00:00\n",
            "3 2016-04-28 00:00:00\n",
            "4 2016-04-29 00:00:00\n",
            "5 2016-04-30 00:00:00\n",
            "6 2016-05-01 00:00:00\n",
            "7 2016-05-02 00:00:00\n",
            "8 2016-05-03 00:00:00\n",
            "9 2016-05-04 00:00:00\n",
            "10 2016-05-05 00:00:00\n",
            "11 2016-05-06 00:00:00\n",
            "12 2016-05-07 00:00:00\n",
            "13 2016-05-08 00:00:00\n",
            "14 2016-05-09 00:00:00\n",
            "15 2016-05-10 00:00:00\n",
            "16 2016-05-11 00:00:00\n",
            "17 2016-05-12 00:00:00\n",
            "18 2016-05-13 00:00:00\n",
            "19 2016-05-14 00:00:00\n",
            "20 2016-05-15 00:00:00\n",
            "21 2016-05-16 00:00:00\n",
            "22 2016-05-17 00:00:00\n",
            "23 2016-05-18 00:00:00\n",
            "24 2016-05-19 00:00:00\n",
            "25 2016-05-20 00:00:00\n",
            "26 2016-05-21 00:00:00\n",
            "27 2016-05-22 00:00:00\n",
            "1 1.023 0.3333333333333333\n",
            "0 2016-04-25 00:00:00\n",
            "1 2016-04-26 00:00:00\n",
            "2 2016-04-27 00:00:00\n",
            "3 2016-04-28 00:00:00\n",
            "4 2016-04-29 00:00:00\n",
            "5 2016-04-30 00:00:00\n",
            "6 2016-05-01 00:00:00\n",
            "7 2016-05-02 00:00:00\n",
            "8 2016-05-03 00:00:00\n",
            "9 2016-05-04 00:00:00\n",
            "10 2016-05-05 00:00:00\n",
            "11 2016-05-06 00:00:00\n",
            "12 2016-05-07 00:00:00\n",
            "13 2016-05-08 00:00:00\n",
            "14 2016-05-09 00:00:00\n",
            "15 2016-05-10 00:00:00\n",
            "16 2016-05-11 00:00:00\n",
            "17 2016-05-12 00:00:00\n",
            "18 2016-05-13 00:00:00\n",
            "19 2016-05-14 00:00:00\n",
            "20 2016-05-15 00:00:00\n",
            "21 2016-05-16 00:00:00\n",
            "22 2016-05-17 00:00:00\n",
            "23 2016-05-18 00:00:00\n",
            "24 2016-05-19 00:00:00\n",
            "25 2016-05-20 00:00:00\n",
            "26 2016-05-21 00:00:00\n",
            "27 2016-05-22 00:00:00\n",
            "2 1.018 0.3333333333333333\n",
            "CPU times: user 8h 6min 9s, sys: 1min 45s, total: 8h 7min 55s\n",
            "Wall time: 7h 51min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yo8V-QD3OPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "ff0ee69b-a369-4f30-bd5e-cd49f4a0cbff"
      },
      "source": [
        "sub.head(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>F</th>\n",
              "      <th>id</th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "      <th>F18</th>\n",
              "      <th>F19</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F22</th>\n",
              "      <th>F23</th>\n",
              "      <th>F24</th>\n",
              "      <th>F25</th>\n",
              "      <th>F26</th>\n",
              "      <th>F27</th>\n",
              "      <th>F28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FOODS_1_001_CA_1_validation</td>\n",
              "      <td>1.098673</td>\n",
              "      <td>1.022820</td>\n",
              "      <td>0.997819</td>\n",
              "      <td>0.875534</td>\n",
              "      <td>1.234516</td>\n",
              "      <td>1.459061</td>\n",
              "      <td>1.459077</td>\n",
              "      <td>1.292348</td>\n",
              "      <td>1.351955</td>\n",
              "      <td>1.137295</td>\n",
              "      <td>1.299196</td>\n",
              "      <td>1.349206</td>\n",
              "      <td>1.839424</td>\n",
              "      <td>1.437884</td>\n",
              "      <td>1.396450</td>\n",
              "      <td>1.186929</td>\n",
              "      <td>1.256992</td>\n",
              "      <td>1.223022</td>\n",
              "      <td>1.333963</td>\n",
              "      <td>1.861751</td>\n",
              "      <td>1.759072</td>\n",
              "      <td>1.333841</td>\n",
              "      <td>1.234098</td>\n",
              "      <td>1.235177</td>\n",
              "      <td>1.153949</td>\n",
              "      <td>1.242508</td>\n",
              "      <td>1.690750</td>\n",
              "      <td>1.758571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FOODS_1_001_CA_2_validation</td>\n",
              "      <td>1.040031</td>\n",
              "      <td>1.308017</td>\n",
              "      <td>0.983890</td>\n",
              "      <td>1.335740</td>\n",
              "      <td>1.336309</td>\n",
              "      <td>1.735975</td>\n",
              "      <td>2.092439</td>\n",
              "      <td>1.297207</td>\n",
              "      <td>1.336175</td>\n",
              "      <td>1.151002</td>\n",
              "      <td>1.262915</td>\n",
              "      <td>1.419095</td>\n",
              "      <td>2.189462</td>\n",
              "      <td>1.545336</td>\n",
              "      <td>1.393283</td>\n",
              "      <td>1.299332</td>\n",
              "      <td>1.398599</td>\n",
              "      <td>1.385259</td>\n",
              "      <td>1.491573</td>\n",
              "      <td>2.216568</td>\n",
              "      <td>2.286248</td>\n",
              "      <td>1.357851</td>\n",
              "      <td>1.249780</td>\n",
              "      <td>1.262961</td>\n",
              "      <td>1.203617</td>\n",
              "      <td>1.347830</td>\n",
              "      <td>1.928699</td>\n",
              "      <td>1.990839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOODS_1_001_CA_3_validation</td>\n",
              "      <td>1.554818</td>\n",
              "      <td>1.496190</td>\n",
              "      <td>1.473606</td>\n",
              "      <td>1.454691</td>\n",
              "      <td>1.581425</td>\n",
              "      <td>1.805980</td>\n",
              "      <td>1.793064</td>\n",
              "      <td>1.620869</td>\n",
              "      <td>1.780457</td>\n",
              "      <td>1.641296</td>\n",
              "      <td>1.875835</td>\n",
              "      <td>2.159771</td>\n",
              "      <td>2.492027</td>\n",
              "      <td>1.883502</td>\n",
              "      <td>1.915424</td>\n",
              "      <td>1.719704</td>\n",
              "      <td>1.873420</td>\n",
              "      <td>1.908869</td>\n",
              "      <td>2.091048</td>\n",
              "      <td>2.880561</td>\n",
              "      <td>3.035407</td>\n",
              "      <td>1.991676</td>\n",
              "      <td>1.900953</td>\n",
              "      <td>1.985316</td>\n",
              "      <td>1.806536</td>\n",
              "      <td>1.909291</td>\n",
              "      <td>2.707650</td>\n",
              "      <td>2.562894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FOODS_1_001_CA_4_validation</td>\n",
              "      <td>0.554983</td>\n",
              "      <td>0.417553</td>\n",
              "      <td>0.421589</td>\n",
              "      <td>0.420868</td>\n",
              "      <td>0.535021</td>\n",
              "      <td>0.612739</td>\n",
              "      <td>0.718413</td>\n",
              "      <td>0.672702</td>\n",
              "      <td>0.731614</td>\n",
              "      <td>0.624331</td>\n",
              "      <td>0.669541</td>\n",
              "      <td>0.608409</td>\n",
              "      <td>0.665055</td>\n",
              "      <td>0.545598</td>\n",
              "      <td>0.580606</td>\n",
              "      <td>0.548967</td>\n",
              "      <td>0.604833</td>\n",
              "      <td>0.634653</td>\n",
              "      <td>0.629666</td>\n",
              "      <td>0.737405</td>\n",
              "      <td>0.691096</td>\n",
              "      <td>0.523849</td>\n",
              "      <td>0.494171</td>\n",
              "      <td>0.506868</td>\n",
              "      <td>0.487351</td>\n",
              "      <td>0.509175</td>\n",
              "      <td>0.670414</td>\n",
              "      <td>0.683448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FOODS_1_001_TX_1_validation</td>\n",
              "      <td>0.316489</td>\n",
              "      <td>0.305510</td>\n",
              "      <td>0.299616</td>\n",
              "      <td>0.299709</td>\n",
              "      <td>0.307175</td>\n",
              "      <td>0.338282</td>\n",
              "      <td>0.370671</td>\n",
              "      <td>0.679834</td>\n",
              "      <td>0.771715</td>\n",
              "      <td>0.672857</td>\n",
              "      <td>0.778826</td>\n",
              "      <td>0.748299</td>\n",
              "      <td>0.813342</td>\n",
              "      <td>0.644049</td>\n",
              "      <td>0.754720</td>\n",
              "      <td>0.756535</td>\n",
              "      <td>0.767674</td>\n",
              "      <td>0.762346</td>\n",
              "      <td>0.779191</td>\n",
              "      <td>0.922532</td>\n",
              "      <td>0.904106</td>\n",
              "      <td>0.655448</td>\n",
              "      <td>0.613595</td>\n",
              "      <td>0.596276</td>\n",
              "      <td>0.551080</td>\n",
              "      <td>0.590111</td>\n",
              "      <td>0.725678</td>\n",
              "      <td>0.685868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>FOODS_1_001_TX_2_validation</td>\n",
              "      <td>0.548314</td>\n",
              "      <td>0.530293</td>\n",
              "      <td>0.517855</td>\n",
              "      <td>0.471678</td>\n",
              "      <td>0.534096</td>\n",
              "      <td>0.726891</td>\n",
              "      <td>0.683201</td>\n",
              "      <td>0.703463</td>\n",
              "      <td>0.779146</td>\n",
              "      <td>0.624083</td>\n",
              "      <td>0.816554</td>\n",
              "      <td>0.789702</td>\n",
              "      <td>0.896251</td>\n",
              "      <td>0.661808</td>\n",
              "      <td>0.678579</td>\n",
              "      <td>0.625236</td>\n",
              "      <td>0.720204</td>\n",
              "      <td>0.653901</td>\n",
              "      <td>0.670175</td>\n",
              "      <td>0.842221</td>\n",
              "      <td>0.788409</td>\n",
              "      <td>0.609723</td>\n",
              "      <td>0.614559</td>\n",
              "      <td>0.612110</td>\n",
              "      <td>0.572989</td>\n",
              "      <td>0.647049</td>\n",
              "      <td>0.793125</td>\n",
              "      <td>0.766678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>FOODS_1_001_TX_3_validation</td>\n",
              "      <td>0.473368</td>\n",
              "      <td>0.405845</td>\n",
              "      <td>0.395354</td>\n",
              "      <td>0.472560</td>\n",
              "      <td>0.485227</td>\n",
              "      <td>0.542841</td>\n",
              "      <td>0.627963</td>\n",
              "      <td>0.644553</td>\n",
              "      <td>0.704863</td>\n",
              "      <td>0.591691</td>\n",
              "      <td>0.664486</td>\n",
              "      <td>0.678166</td>\n",
              "      <td>0.751235</td>\n",
              "      <td>0.563344</td>\n",
              "      <td>0.600821</td>\n",
              "      <td>0.567761</td>\n",
              "      <td>0.598909</td>\n",
              "      <td>0.607372</td>\n",
              "      <td>0.667330</td>\n",
              "      <td>0.746915</td>\n",
              "      <td>0.710848</td>\n",
              "      <td>0.576511</td>\n",
              "      <td>0.543512</td>\n",
              "      <td>0.553840</td>\n",
              "      <td>0.512856</td>\n",
              "      <td>0.554489</td>\n",
              "      <td>0.665139</td>\n",
              "      <td>0.631681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>FOODS_1_001_WI_1_validation</td>\n",
              "      <td>0.384921</td>\n",
              "      <td>0.474208</td>\n",
              "      <td>0.412361</td>\n",
              "      <td>0.416860</td>\n",
              "      <td>0.497414</td>\n",
              "      <td>0.781901</td>\n",
              "      <td>0.891231</td>\n",
              "      <td>0.788247</td>\n",
              "      <td>0.808481</td>\n",
              "      <td>0.721102</td>\n",
              "      <td>0.911687</td>\n",
              "      <td>0.865463</td>\n",
              "      <td>1.085459</td>\n",
              "      <td>0.813322</td>\n",
              "      <td>0.757734</td>\n",
              "      <td>0.719601</td>\n",
              "      <td>0.789476</td>\n",
              "      <td>0.805179</td>\n",
              "      <td>0.920705</td>\n",
              "      <td>1.316374</td>\n",
              "      <td>1.230173</td>\n",
              "      <td>0.836360</td>\n",
              "      <td>0.789447</td>\n",
              "      <td>0.818003</td>\n",
              "      <td>0.791189</td>\n",
              "      <td>0.943223</td>\n",
              "      <td>1.507604</td>\n",
              "      <td>1.167656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FOODS_1_001_WI_2_validation</td>\n",
              "      <td>0.429086</td>\n",
              "      <td>0.469116</td>\n",
              "      <td>0.457969</td>\n",
              "      <td>0.419200</td>\n",
              "      <td>0.496798</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>0.534624</td>\n",
              "      <td>0.623794</td>\n",
              "      <td>0.715672</td>\n",
              "      <td>0.534169</td>\n",
              "      <td>0.615390</td>\n",
              "      <td>0.664011</td>\n",
              "      <td>0.728630</td>\n",
              "      <td>0.581457</td>\n",
              "      <td>0.612398</td>\n",
              "      <td>0.561860</td>\n",
              "      <td>0.634450</td>\n",
              "      <td>0.602782</td>\n",
              "      <td>0.582271</td>\n",
              "      <td>0.731807</td>\n",
              "      <td>0.700585</td>\n",
              "      <td>0.526748</td>\n",
              "      <td>0.521034</td>\n",
              "      <td>0.551505</td>\n",
              "      <td>0.524909</td>\n",
              "      <td>0.532879</td>\n",
              "      <td>0.629011</td>\n",
              "      <td>0.579230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>FOODS_1_001_WI_3_validation</td>\n",
              "      <td>0.358054</td>\n",
              "      <td>0.358120</td>\n",
              "      <td>0.370933</td>\n",
              "      <td>0.366427</td>\n",
              "      <td>0.425022</td>\n",
              "      <td>0.686437</td>\n",
              "      <td>0.531013</td>\n",
              "      <td>0.619840</td>\n",
              "      <td>0.734767</td>\n",
              "      <td>0.590584</td>\n",
              "      <td>0.699504</td>\n",
              "      <td>0.767726</td>\n",
              "      <td>0.723573</td>\n",
              "      <td>0.586869</td>\n",
              "      <td>0.564464</td>\n",
              "      <td>0.521326</td>\n",
              "      <td>0.585736</td>\n",
              "      <td>0.588600</td>\n",
              "      <td>0.617101</td>\n",
              "      <td>0.797293</td>\n",
              "      <td>0.732423</td>\n",
              "      <td>0.520050</td>\n",
              "      <td>0.501341</td>\n",
              "      <td>0.499891</td>\n",
              "      <td>0.485642</td>\n",
              "      <td>0.542018</td>\n",
              "      <td>0.681906</td>\n",
              "      <td>0.591939</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "F                           id        F1        F2        F3        F4  \\\n",
              "0  FOODS_1_001_CA_1_validation  1.098673  1.022820  0.997819  0.875534   \n",
              "1  FOODS_1_001_CA_2_validation  1.040031  1.308017  0.983890  1.335740   \n",
              "2  FOODS_1_001_CA_3_validation  1.554818  1.496190  1.473606  1.454691   \n",
              "3  FOODS_1_001_CA_4_validation  0.554983  0.417553  0.421589  0.420868   \n",
              "4  FOODS_1_001_TX_1_validation  0.316489  0.305510  0.299616  0.299709   \n",
              "5  FOODS_1_001_TX_2_validation  0.548314  0.530293  0.517855  0.471678   \n",
              "6  FOODS_1_001_TX_3_validation  0.473368  0.405845  0.395354  0.472560   \n",
              "7  FOODS_1_001_WI_1_validation  0.384921  0.474208  0.412361  0.416860   \n",
              "8  FOODS_1_001_WI_2_validation  0.429086  0.469116  0.457969  0.419200   \n",
              "9  FOODS_1_001_WI_3_validation  0.358054  0.358120  0.370933  0.366427   \n",
              "\n",
              "F        F5        F6        F7        F8        F9       F10       F11  \\\n",
              "0  1.234516  1.459061  1.459077  1.292348  1.351955  1.137295  1.299196   \n",
              "1  1.336309  1.735975  2.092439  1.297207  1.336175  1.151002  1.262915   \n",
              "2  1.581425  1.805980  1.793064  1.620869  1.780457  1.641296  1.875835   \n",
              "3  0.535021  0.612739  0.718413  0.672702  0.731614  0.624331  0.669541   \n",
              "4  0.307175  0.338282  0.370671  0.679834  0.771715  0.672857  0.778826   \n",
              "5  0.534096  0.726891  0.683201  0.703463  0.779146  0.624083  0.816554   \n",
              "6  0.485227  0.542841  0.627963  0.644553  0.704863  0.591691  0.664486   \n",
              "7  0.497414  0.781901  0.891231  0.788247  0.808481  0.721102  0.911687   \n",
              "8  0.496798  0.563328  0.534624  0.623794  0.715672  0.534169  0.615390   \n",
              "9  0.425022  0.686437  0.531013  0.619840  0.734767  0.590584  0.699504   \n",
              "\n",
              "F       F12       F13       F14       F15       F16       F17       F18  \\\n",
              "0  1.349206  1.839424  1.437884  1.396450  1.186929  1.256992  1.223022   \n",
              "1  1.419095  2.189462  1.545336  1.393283  1.299332  1.398599  1.385259   \n",
              "2  2.159771  2.492027  1.883502  1.915424  1.719704  1.873420  1.908869   \n",
              "3  0.608409  0.665055  0.545598  0.580606  0.548967  0.604833  0.634653   \n",
              "4  0.748299  0.813342  0.644049  0.754720  0.756535  0.767674  0.762346   \n",
              "5  0.789702  0.896251  0.661808  0.678579  0.625236  0.720204  0.653901   \n",
              "6  0.678166  0.751235  0.563344  0.600821  0.567761  0.598909  0.607372   \n",
              "7  0.865463  1.085459  0.813322  0.757734  0.719601  0.789476  0.805179   \n",
              "8  0.664011  0.728630  0.581457  0.612398  0.561860  0.634450  0.602782   \n",
              "9  0.767726  0.723573  0.586869  0.564464  0.521326  0.585736  0.588600   \n",
              "\n",
              "F       F19       F20       F21       F22       F23       F24       F25  \\\n",
              "0  1.333963  1.861751  1.759072  1.333841  1.234098  1.235177  1.153949   \n",
              "1  1.491573  2.216568  2.286248  1.357851  1.249780  1.262961  1.203617   \n",
              "2  2.091048  2.880561  3.035407  1.991676  1.900953  1.985316  1.806536   \n",
              "3  0.629666  0.737405  0.691096  0.523849  0.494171  0.506868  0.487351   \n",
              "4  0.779191  0.922532  0.904106  0.655448  0.613595  0.596276  0.551080   \n",
              "5  0.670175  0.842221  0.788409  0.609723  0.614559  0.612110  0.572989   \n",
              "6  0.667330  0.746915  0.710848  0.576511  0.543512  0.553840  0.512856   \n",
              "7  0.920705  1.316374  1.230173  0.836360  0.789447  0.818003  0.791189   \n",
              "8  0.582271  0.731807  0.700585  0.526748  0.521034  0.551505  0.524909   \n",
              "9  0.617101  0.797293  0.732423  0.520050  0.501341  0.499891  0.485642   \n",
              "\n",
              "F       F26       F27       F28  \n",
              "0  1.242508  1.690750  1.758571  \n",
              "1  1.347830  1.928699  1.990839  \n",
              "2  1.909291  2.707650  2.562894  \n",
              "3  0.509175  0.670414  0.683448  \n",
              "4  0.590111  0.725678  0.685868  \n",
              "5  0.647049  0.793125  0.766678  \n",
              "6  0.554489  0.665139  0.631681  \n",
              "7  0.943223  1.507604  1.167656  \n",
              "8  0.532879  0.629011  0.579230  \n",
              "9  0.542018  0.681906  0.591939  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQfH3n3i3OPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "474b66d3-b501-44bb-9eb2-55368bb39fe2"
      },
      "source": [
        "sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60980, 30490)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrKpuLhu3OPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11448750-1af5-4ff1-b10c-44c9d7eaba72"
      },
      "source": [
        "sub.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60980, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaZSV-yH3OPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAqaS0uD3OPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}